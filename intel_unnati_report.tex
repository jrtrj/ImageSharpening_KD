\documentclass[12pt,a4paper]{article}

% --- PACKAGES FOR LAYOUT AND STYLING ---
\usepackage[margin=1in, headheight=14.5pt]{geometry} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{times}

% --- PARAGRAPH FLOW OPTIMIZATION ---
\widowpenalty=10000
\clubpenalty=10000

% --- HYPERLINK SETUP ---
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=blue,
    pdftitle={Image Sharpening with Knowledge Distillation},
}

% --- HEADER AND FOOTER SETUP ---
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[L]{INTEL-UNNATI AIML JOURNAL}
\fancyhead[R]{Code:IUCP-2024}
\cfoot{\thepage}

% --- CUSTOM FOOTER FOR FIRST PAGE ---
\fancypagestyle{firstpage}{
  \fancyhf{}
  \fancyfoot[L]{Â© Intel-unnati}
  \fancyfoot[R]{Project Report}
}

% --- CODE LISTING STYLE ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.05,0.05,0.05}
\definecolor{stringcolor}{rgb}{0.9,0.5,0.2}
\definecolor{keywordcolor}{rgb}{0.2,0.6,1.0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keywordcolor},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{stringcolor},
    basicstyle=\ttfamily\footnotesize\color{white}, 
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- DOCUMENT METADATA ---
\title{Image Sharpening using a ResNet-based Architecture and Knowledge Distillation}
\author[1]{Jerit Reji}
\author[1]{Joel Mathew Samuel}
\author[1]{Sebastian Abraham}
\affil[1]{Saintgits Group of Institutions, Kottayam, Kerala}
\date{July 2025}

\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
    \thispagestyle{firstpage}
    \newgeometry{margin=0.8in}
    
    \begin{minipage}{0.5\textwidth}
        \flushleft \includegraphics[width=1.2in]{intel_logo.png}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \flushright \includegraphics[width=2.0in]{saintgits_logo.png}
    \end{minipage}
    
    \vfill 
    \centering
    {\Huge\bfseries Image Sharpening using a ResNet-based Architecture and Knowledge Distillation \par}
    \vspace{1.5cm} 
    {\Large Jerit Reji, Joel Mathew Samuel, Sebastian Abraham \par}
    \vspace{0.5cm}
    {\large \textit{Saintgits Group of Institutions, Kottayam, Kerala}\par}
    \vfill 
    
\end{titlepage}

\restoregeometry
\pagestyle{fancy} 

% --- ABSTRACT ---
\begin{abstract}
\noindent This report details the implementation of an image sharpening system using knowledge distillation. A deep "teacher" model, based on a Residual Network (ResNet), is trained on the DIV2K dataset to restore degraded images. The training objective for this model is a composite loss function combining Structural Similarity (SSIM), Mean Squared Error (MSE), and a VGG-based Perceptual Loss. Subsequently, a smaller, computationally efficient "student" Convolutional Neural Network (CNN), built with depthwise separable convolutions, is trained to mimic the teacher's output using a distillation loss. The final student model's performance is evaluated against the teacher's, demonstrating the effectiveness of transferring knowledge to a compact architecture for practical applications.
\end{abstract}

\vspace{0.5cm}
\textbf{Keywords:} Image Sharpening, Deep Learning, Computer Vision, ResNet, Knowledge Distillation, SSIM, Perceptual Loss, PyTorch, Depthwise Separable Convolution.
\vspace{1cm}

\section{Introduction}
Restoring detail in images degraded by blur, noise, or downscaling is a fundamental problem in computer vision. While deep learning models can learn complex restoration transforms, state-of-the-art architectures are often too large and computationally intensive for practical deployment on consumer devices. This creates a conflict between model performance and efficiency.

This project addresses this challenge by implementing a teacher-student framework based on knowledge distillation. The primary goal is to transfer the image restoration capability of a large, high-performance "teacher" network into a much smaller and faster "student" network. This approach aims to produce a final model that is both effective at image sharpening and suitable for resource-constrained environments.

\section{Libraries Used}
The implementation of this project relies on several key Python libraries for deep learning, image processing, and data handling. The primary packages used are listed below.
\begin{verbatim}
torch
torchvision
pytorch-msssim
albumentations
opencv-python
scikit-image
numpy
matplotlib
\end{verbatim}

\newpage
\section{Methodology}

\subsection{Dataset and Augmentation}
The DIV2K dataset, containing high-resolution images, was used as the basis for training. A custom data pipeline was created to generate pairs of degraded and ground-truth images on-the-fly using the \texttt{albumentations} library. The degradation transform applies a random combination of the following:
\begin{itemize}
    \item \textbf{Blur:} Gaussian or Motion blur, with a kernel size between 3 and 7.
    \item \textbf{Downscaling:} Resizing the image to between 60\% and 80\% of its original size.
    \item \textbf{Noise:} Adding Gaussian noise with a variance between 10 and 50.
\end{itemize}

\subsection{Model Architecture}
The core of the project is a teacher-student framework.

\subsubsection{Teacher Model: ResNetSharpen}
The teacher is a ResNet architecture designed for high-capacity learning. It features an input convolution, 8 residual blocks, and an output convolution. The residual connection is applied before the final activation within each block. A global residual connection adds the input image to the network's output, allowing the model to focus on learning the sharpening details.

\begin{lstlisting}[language=Python, caption={Updated ResidualBlock for Teacher Model}]
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return self.relu(x + out)
\end{lstlisting}

\subsubsection{Student Model: StudentCNN}
The student model is an efficient CNN designed for fast inference. Its core components are blocks built from **depthwise separable convolutions**, which significantly reduce the number of parameters and computational cost compared to standard convolutions. This is achieved by splitting the convolution into two steps: a depthwise step that applies a single filter per input channel, and a pointwise step (a 1x1 convolution) that combines the outputs.

\begin{lstlisting}[language=Python, caption={Depthwise Separable Convolution Module}]
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.act(self.pointwise(self.depthwise(x)))
\end{lstlisting}

These efficient convolutions are used in a `StudentBlock` with a residual connection. The final `StudentCNN` consists of an entry convolution, three stacked `StudentBlock` modules, and an exit convolution, all wrapped in a global residual connection.

\begin{lstlisting}[language=Python, caption={Updated StudentCNN Architecture}]
class StudentBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = DepthwiseSeparableConv(channels, channels)
        self.conv2 = DepthwiseSeparableConv(channels, channels)

    def forward(self, x):
        return x + self.conv2(self.conv1(x))

class StudentCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.entry = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.blocks = nn.Sequential(
            StudentBlock(32),
            StudentBlock(32),
            StudentBlock(32)
        )
        self.exit = nn.Conv2d(32, 3, kernel_size=3, padding=1)

    def forward(self, x):
        out = self.relu(self.entry(x))
        out = self.blocks(out)
        out = self.exit(out)
        return x + out
\end{lstlisting}

\subsection{Loss Functions}
The loss functions remain consistent with the project's goal of balancing perceptual and pixel-level accuracy. The teacher is trained with a composite loss (`TotalLoss`), and the student is trained with `distillation_loss`.

\newpage
\section{Results and Discussion}
Both models were trained for 20 epochs using the Adam optimizer (learning rate 1e-4) with a Cosine Annealing scheduler. The performance was evaluated on a held-out validation set using the SSIM metric.

The final quantitative results from the updated models are:
\begin{itemize}
    \item \textbf{Final Teacher Model SSIM:} 0.6344
    \item \textbf{Final Distilled Student SSIM:} 0.6133
\end{itemize}

The distilled student model, built with efficient depthwise separable convolutions, achieves **96.7\%** of the teacher model's performance. This result is a strong validation of the knowledge distillation process, showing that the performance of a large, complex network can be successfully transferred to a highly optimized, lightweight architecture with minimal degradation in quality.

Qualitative analysis, presented in Figure \ref{fig:visual_results}, visually confirms these findings. The student model's output demonstrates significant restoration of detail and is visually comparable to the teacher's output.

\begin{figure}[htbp]
    \centering
    % USER ACTION: Make sure to replace this with a new screenshot from your latest notebook.
    \includegraphics[width=\textwidth]{figure_visual_results.png}
    \caption{Visual comparison of model outputs on the validation set. Each row shows the degraded input, the student's sharpened output (with SSIM score), the teacher's output, and the original ground truth.}
    \label{fig:visual_results}
\end{figure}

\newpage
\section{Conclusion}
This project successfully developed and evaluated an image sharpening system using knowledge distillation. A high-capacity ResNet teacher model was trained, and its knowledge was transferred to a lightweight student CNN composed of depthwise separable convolutions. The final student model achieved an SSIM of **0.6133**, recovering 96.7\% of the teacher model's performance (0.6344 SSIM) with a substantially more efficient architecture.

The results confirm that knowledge distillation, when paired with efficient model design, is a highly effective strategy for creating deployable computer vision models that balance performance with computational cost.

\newpage
% --- GITHUB AND ACKNOWLEDGMENTS ---
\section*{GitHub Repository}
The complete source code for this project, including training notebooks and model implementations, is publicly available on GitHub at the following repository:
\url{https://github.com/jrtrj/ImageSharpening_KD/}

\vspace{1cm}

\section*{Acknowledgments}
We would like to express our sincere gratitude to our project mentor, Dr. Anju Pratap, for her invaluable guidance, constant support, and insightful feedback throughout the duration of this research. Her expertise was instrumental in shaping the direction of this work. We are also deeply indebted to our college, Saintgits College of Engineering and Technology, for providing the necessary resources and academic environment to facilitate this project.

% --- REFERENCES SECTION ---
\vspace{2cm}
\hrule
\vspace{0.5cm}
\begin{thebibliography}{9}
    \bibitem{he2016deep}
    K. He, X. Zhang, S. Ren, and J. Sun,
    \textit{Deep residual learning for image recognition},
    in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
    
    \bibitem{hinton2015distilling}
    G. Hinton, O. Vinyals, and J. Dean,
    \textit{Distilling the knowledge in a neural network},
    arXiv preprint arXiv:1503.02531, 2015.

    \bibitem{paszke2019pytorch}
    A. Paszke et al.,
    \textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    in Advances in Neural Information Processing Systems, 2019.

\end{thebibliography}

\end{document}
