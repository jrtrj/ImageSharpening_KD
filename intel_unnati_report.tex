\documentclass[12pt,a4paper]{article}

% --- PACKAGES FOR LAYOUT AND STYLING ---
\usepackage[margin=1in, headheight=14.5pt]{geometry} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{times}

% --- PARAGRAPH FLOW OPTIMIZATION ---
\widowpenalty=10000
\clubpenalty=10000

% --- HYPERLINK SETUP ---
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=blue, % Keep URLs blue for visibility
    pdftitle={Image Sharpening with Knowledge Distillation},
}

% --- HEADER AND FOOTER SETUP ---
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[L]{INTEL-UNNATI AI\&ML JOURNAL}
\cfoot{\thepage}

% --- CUSTOM FOOTER FOR FIRST PAGE ---
\fancypagestyle{firstpage}{
  \fancyhf{}
  \fancyfoot[L]{Â© Intel-unnati}
  \fancyfoot[R]{Project Report}
}

% --- CODE LISTING STYLE ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.05,0.05,0.05}
\definecolor{stringcolor}{rgb}{0.9,0.5,0.2}
\definecolor{keywordcolor}{rgb}{0.2,0.6,1.0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keywordcolor},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{stringcolor},
    basicstyle=\ttfamily\footnotesize\color{white}, 
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- DOCUMENT METADATA ---
\title{Image Sharpening using a ResNet-based Architecture and Knowledge Distillation}
\author[1]{Jerit Reji}
\author[1]{Joel Mathew Samuel}
\author[1]{Sebastian Abraham}
\affil[1]{Saintgits Group of Institutions, Kottayam, Kerala}
\date{July 2025}

\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
    \thispagestyle{firstpage}
    \newgeometry{margin=0.8in}
    
    \begin{minipage}{0.5\textwidth}
        \flushleft \includegraphics[width=1.2in]{intel_logo.png}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \flushright \includegraphics[width=2.0in]{saintgits_logo.png}
    \end{minipage}
    
    \vfill 
    \centering
    {\Huge\bfseries Image Sharpening using a ResNet-based Architecture and Knowledge Distillation \par}
    \vspace{1.5cm} 
    {\Large Jerit Reji, Joel Mathew Samuel, Sebastian Abraham \par}
    \vspace{0.5cm}
    {\large \textit{Saintgits Group of Institutions, Kottayam, Kerala}\par}
    \vfill 
    
\end{titlepage}

\restoregeometry
\pagestyle{fancy} 

% --- ABSTRACT ---
\begin{abstract}
\noindent This report details the implementation of an image sharpening system using knowledge distillation. A deep "teacher" model, based on a Residual Network (ResNet), is trained on the DIV2K dataset to restore degraded images. The training objective for this model is a composite loss function combining Structural Similarity (SSIM), Mean Squared Error (MSE), and a VGG-based Perceptual Loss. Subsequently, a smaller, computationally efficient "student" Convolutional Neural Network (CNN) is trained to mimic the teacher's output using a distillation loss. The final student model's performance is evaluated against the teacher's, demonstrating the effectiveness of transferring knowledge to a compact architecture for practical applications.
\end{abstract}

\vspace{0.5cm}
\textbf{Keywords:} Image Sharpening, Deep Learning, Computer Vision, ResNet, Knowledge Distillation, SSIM, Perceptual Loss, PyTorch, Model Optimization.
\vspace{1cm}

\section{Introduction}
Restoring detail in images degraded by blur, noise, or downscaling is a fundamental problem in computer vision. While deep learning models can learn complex restoration transforms, state-of-the-art architectures are often too large and computationally intensive for practical deployment on consumer devices. This creates a conflict between model performance and efficiency.

This project addresses this challenge by implementing a teacher-student framework based on knowledge distillation. The primary goal is to transfer the image restoration capability of a large, high-performance "teacher" network into a much smaller and faster "student" network. This approach aims to produce a final model that is both effective at image sharpening and suitable for resource-constrained environments.

\newpage
\section{Methodology}

\subsection{Dataset and Augmentation}
The DIV2K dataset, containing high-resolution images, was used as the basis for training. A custom data pipeline was created to generate pairs of degraded and ground-truth images on-the-fly using the \texttt{albumentations} library. The degradation transform applies a random combination of the following:
\begin{itemize}
    \item \textbf{Blur:} Gaussian or Motion blur, with a kernel size between 3 and 7.
    \item \textbf{Downscaling:} Resizing the image to between 60\% and 80\% of its original size.
    \item \textbf{Noise:} Adding Gaussian noise with a variance between 10 and 50.
\end{itemize}

\subsection{Model Architecture}
The core of the project is a teacher-student framework.

\subsubsection{Teacher Model: ResNetSharpen}
The teacher is a ResNet-inspired architecture designed for high-capacity learning. It features an input convolution layer mapping 3 to 64 channels, followed by 8 residual blocks, and an output convolution layer mapping 64 back to 3 channels. A global residual connection adds the input image to the network's output, allowing the model to focus on learning the sharpening details rather than reconstructing the entire image.

\begin{lstlisting}[language=Python, caption={ResNetSharpen Teacher Model Architecture}]
class ResNetSharpen(nn.Module):
    def __init__(self, num_blocks=8):
        super(ResNetSharpen, self).__init__()
        self.conv_in = nn.Conv2d(3, 64, 3, 1, 1)
        self.residual_layers = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])
        self.conv_out = nn.Conv2d(64, 3, 3, 1, 1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        out = self.relu(self.conv_in(x))
        out = self.residual_layers(out)
        out = self.conv_out(out)
        return identity + out
\end{lstlisting}

\subsubsection{Student Model: StudentCNN}
The student is a lightweight CNN with minimal layers, designed for fast inference. It consists of two 3x3 convolutional layers and also employs a global residual connection.

\begin{lstlisting}[language=Python, caption={StudentCNN Architecture}]
class StudentCNN(nn.Module):
    def __init__(self):
        super(StudentCNN, self).__init__()
        self.body = nn.Sequential(
            nn.Conv2d(3, 32, 3, 1, 1), 
            nn.ReLU(True), 
            nn.Conv2d(32, 32, 3, 1, 1), 
            nn.ReLU(True)
        )
    def forward(self, x): 
        return x + self.body(x)
\end{lstlisting}

\subsection{Loss Functions}
\subsubsection{Teacher Training Loss}
The teacher model is trained using a composite loss function to balance pixel accuracy with perceptual quality. It is a weighted sum of an SSIM/MSE loss and a VGG-based Perceptual Loss, with weights of `ssim_w=0.8` and `perceptual_w=0.01`. The Perceptual Loss compares high-level features from a pre-trained VGG19 network to better align with human visual assessment.

\subsubsection{Knowledge Distillation Loss}
The student model is trained using a distillation loss. This loss is a weighted average of two components: the mean squared error between the student's and the teacher's outputs, and the mean squared error between the student's output and the ground-truth image. The weighting factor `alpha` is set to 0.75, placing more emphasis on mimicking the teacher.
\begin{lstlisting}[language=Python, caption={Knowledge Distillation Loss Function}]
def distillation_loss(s_out, t_out, target, alpha=0.75):
    loss_teacher = nn.MSELoss()(s_out, t_out)
    loss_gt = nn.MSELoss()(s_out, target)
    return alpha * loss_teacher + (1 - alpha) * loss_gt
\end{lstlisting}

\newpage
\section{Results and Discussion}
Both the teacher and student models were trained for 20 epochs using the Adam optimizer with a learning rate of 1e-4 and a Cosine Annealing scheduler. The final performance was evaluated on a held-out validation set using the Structural Similarity Index (SSIM) as the primary metric.

The final quantitative results are as follows:
\begin{itemize}
    \item \textbf{Final Teacher Model SSIM:} 0.6312
    \item \textbf{Final Distilled Student SSIM:} 0.6061
\end{itemize}

The distilled student model achieves over 96\% of the teacher model's performance in terms of SSIM score. This demonstrates a highly successful knowledge transfer, as the significantly smaller student network retains the vast majority of the larger model's capability.

Qualitative analysis, shown in Figure \ref{fig:visual_results}, supports this conclusion. The images produced by the student model show a clear visual improvement over the degraded inputs and are nearly indistinguishable from those produced by the much larger teacher model.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figure_visual_results.png}
    \caption{Visual comparison of model outputs on the validation set. Each row shows the degraded input, the student's sharpened output (with SSIM score), the teacher's output, and the original ground truth.}
    \label{fig:visual_results}
\end{figure}

\newpage
\section{Model Observations}
During development, an alternative teacher architecture based on DnCNN (Denoising Convolutional Neural Network) was evaluated. This model was trained for 10 epochs using only Mean Squared Error (MSE) as the loss function. The implementation for this approach is publicly available on GitHub.\footnote{\href{https://github.com/jrtrj/ImageSharpening_KD/tree/dcnn}{https://github.com/jrtrj/ImageSharpening\_KD/tree/dcnn}}

\subsubsection{DnCNN Performance}
The DnCNN-based teacher achieved a final average SSIM of 0.7327 on the validation set. The student model trained via distillation from this teacher achieved an SSIM of 0.7386. Quantitatively, these SSIM scores are higher than those achieved by the ResNet-based models.

\subsubsection{Limitations and Justification for ResNet}
Despite the higher SSIM scores, the DnCNN approach was not selected for the final project. The primary limitation was its reliance solely on MSE for training. MSE optimizes for pixel-wise average error, which is known to often produce overly smooth images and does not reliably correlate with human perception of image quality.

The `ResNetSharpen` model was ultimately chosen for its more sophisticated training objective. Its composite loss function, incorporating a VGG-based Perceptual Loss, is specifically designed to generate results that are more visually plausible to a human observer. For image restoration tasks, prioritizing perceptual quality over a singular numerical metric like SSIM often yields more desirable results. Therefore, the ResNet-based approach was deemed superior for the project's goal of producing visually appealing sharpened images.

\newpage
\section{Conclusion}
This project successfully implemented a knowledge distillation pipeline for image sharpening. After evaluating multiple architectures, a high-capacity, ResNet-based teacher model was selected and trained on a composite loss function to prioritize perceptual quality. Its knowledge was then effectively transferred to a lightweight student CNN. The final student model achieved an SSIM of 0.6061, recovering 96\% of the teacher model's performance (0.6312 SSIM) with a substantially smaller architecture.

The results confirm that knowledge distillation is a highly effective and practical strategy for model compression in computer vision. It allows for the creation of efficient, deployable models for tasks like image restoration without a significant compromise in output quality.

\newpage
% --- REFERENCES SECTION ---
\hrule
\vspace{0.5cm}
\begin{thebibliography}{9}
    \bibitem{he2016deep}
    K. He, X. Zhang, S. Ren, and J. Sun,
    \textit{Deep residual learning for image recognition},
    in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
    
    \bibitem{hinton2015distilling}
    G. Hinton, O. Vinyals, and J. Dean,
    \textit{Distilling the knowledge in a neural network},
    arXiv preprint arXiv:1503.02531, 2015.

    \bibitem{paszke2019pytorch}
    A. Paszke et al.,
    \textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    in Advances in Neural Information Processing Systems, 2019.

\end{thebibliography}

\end{document}
